{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-bcHjIs1oP1"
      },
      "source": [
        "## 03_01 Tokenisation\n",
        "\n",
        "La tokenisation fait référence à la conversion d'une chaîne de texte en jetons individuels. Les jetons peuvent être des mots ou des ponctuations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import silhouette_score\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "#Lire le fichier de base dans une variable de texte brut\n",
        "dfs = pd.read_excel(\"/content/data.xlsx\", sheet_name=None)\n",
        "df2019 = dfs['2019']\n",
        "\n",
        "# Convertir les dates en format datetime et normaliser\n",
        "df2019['Date de remplissage de la fiche'] = pd.to_datetime(df2019['Date de remplissage de la fiche'], errors='coerce').dt.normalize()\n",
        "\n",
        "df2019['Date de remplissage de la fiche'] = df2019['Date de remplissage de la fiche'].fillna(df2019['Date de remplissage de la fiche'].mode()[0])\n",
        "df2019['Date de naissance'] = pd.to_datetime(df2019['Date de naissance'], errors='coerce').dt.normalize()\n",
        "df2019['Date de naissance'] = df2019['Date de naissance'].fillna(df2019['Date de naissance'].mode()[0])\n",
        "df2019['Si oui preciser la date du dernier don.'] = pd.to_datetime(df2019['Si oui preciser la date du dernier don.'], errors='coerce').dt.normalize()\n",
        "# Calculer l'âge s\n",
        "\n",
        "df2019['Age'] = df2019.apply( lambda x: x['Date de remplissage de la fiche'].year - x['Date de naissance'].year ,\n",
        "    axis=1\n",
        ").astype('Int64')\n",
        "\n",
        "# Afficher les 5 premières valeurs de la colonne Age\n",
        "ages = df2019.pop('Age')\n",
        "df2019.insert(0,'Age',ages)\n",
        "df2 = df2019.copy()\n",
        "#print(df2019[['Age','Date de remplissage de la fiche','Date de naissance']].head(15))\n",
        "df2019 = df2019.drop('Date de naissance', axis =1)\n",
        "df_vol = dfs['Volontaire']\n",
        "df_vol = df_vol.drop(['ID','Horodateur'],axis = 1)\n",
        "df_vol['Date de remplissage de la fiche'] = np.nan\n",
        "df_vol.columns = df2019.columns\n",
        "\n",
        "\n",
        "df = pd.concat([df_vol,df2019],ignore_index = True)\n",
        "imputer = SimpleImputer(strategy = 'constant',fill_value = 0)\n",
        "df.columns = ['Date de remplissage de la fiche','Age',\"Niveau d'etude\", 'Genre', 'Taille', 'Poids',\n",
        "       'Situation Matrimoniale (SM)', 'Profession',\n",
        "       'Arrondissement de résidence', 'Quartier de Résidence', 'Nationalité',\n",
        "       'Religion', 'A-t-il (elle) déjà donné le sang',\n",
        "       'Si oui preciser la date du dernier don.', 'Taux d’hémoglobine',\n",
        "       'ÉLIGIBILITÉ AU DON.',\n",
        "       'Est sous anti-biothérapie',\n",
        "       'Taux d’hémoglobine bas',\n",
        "       'date de dernier Don < 3 mois',\n",
        "       'IST récente (Exclu VIH, Hbs, Hcv)',\n",
        "       'DDR',\n",
        "       'DDR<14',\n",
        "       'Allaitement',\n",
        "       'A accoucher ces 6 derniers mois',\n",
        "       'Interruption de grossesse  ces 06 derniers mois',\n",
        "       'est enceinte',\n",
        "       'Autre raisons ,preciser', 'Sélectionner \"ok\" pour envoyer',\n",
        "       'Antécédent de transfusion',\n",
        "       'Porteur(HIV,hbs,hcv)',\n",
        "       'Opéré',\n",
        "       'Drepanocytaire',       'Diabétique',\n",
        "       'Hypertendus',\n",
        "       'Asthmatiques',\n",
        "       'Cardiaque',\n",
        "       'Tatoué',\n",
        "       'Scarifié',\n",
        "       'Si autres raison preciser']\n",
        "\n",
        "\n",
        "col = ['ÉLIGIBILITÉ AU DON.',\n",
        "       'Est sous anti-biothérapie',\n",
        "       'Taux d’hémoglobine bas',\n",
        "       'date de dernier Don < 3 mois',\n",
        "       'IST récente (Exclu VIH, Hbs, Hcv)',\n",
        "       'DDR<14',\n",
        "       'Allaitement',\n",
        "       'A accoucher ces 6 derniers mois',\n",
        "       'Interruption de grossesse  ces 06 derniers mois',\n",
        "       'est enceinte',\n",
        "       'Antécédent de transfusion',\n",
        "       'Porteur(HIV,hbs,hcv)',\n",
        "       'Opéré',\n",
        "       'Drepanocytaire',\n",
        "       'Diabétique',\n",
        "       'Hypertendus',\n",
        "       'Asthmatiques',\n",
        "       'Cardiaque',\n",
        "       'Tatoué',\n",
        "       'Scarifié'\n",
        "       ]\n",
        "for c in col[1:] :\n",
        "    df[c] = df[c].apply(lambda x : 1 if x == 'Oui' else 0 )\n",
        "df['ÉLIGIBILITÉ AU DON.'] = df['ÉLIGIBILITÉ AU DON.'].apply(lambda x : 1 if x == 'Eligible' else 2 if x == 'Temporairement Non-eligible' else 0 )\n",
        "df_cat = df[col]\n",
        "col = df_cat.columns\n",
        "df_cat = imputer.fit_transform(df_cat)\n",
        "df_cat = pd.DataFrame(df_cat, columns = col)"
      ],
      "metadata": {
        "id": "Djmavr0_gthy",
        "outputId": "187952bd-035f-405d-9b95-ca3634c2c14e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-509f6f761b55>:18: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df2019['Date de remplissage de la fiche'] = pd.to_datetime(df2019['Date de remplissage de la fiche'], errors='coerce').dt.normalize()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "c0TZBmvZ1oP_",
        "outputId": "413da15d-28a5-42c6-d180-7808d666c132",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "expected string or bytes-like object, got 'float'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-fc6e2d8a378f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Extraction des tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Autres raisons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf_autres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens indisponible'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;34m=\u001b[0m\u001b[0mdf_autres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Autre raisons ,preciser'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdf_autres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens ineligible'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;34m=\u001b[0m\u001b[0mdf_autres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Si autres raison préciser'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Token List : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_autres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4922\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4923\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4924\u001b[0;31m         ).apply()\n\u001b[0m\u001b[1;32m   4925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4926\u001b[0m     def _reindex_indexer(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0;31m# self.func is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;31m#  Categorical (GH51645).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m         mapped = obj._map_values(\n\u001b[0m\u001b[1;32m   1508\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         return lib.map_infer_mask(\n",
            "\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     return [\n\u001b[1;32m    144\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[1;32m    119\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1278\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m         \"\"\"\n\u001b[0;32m-> 1280\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m         \"\"\"\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_last_whitespace_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m         \"\"\"\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_last_whitespace_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1326\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1328\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1329\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1455\u001b[0m         \"\"\"\n\u001b[1;32m   1456\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1457\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msentence1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1458\u001b[0m             \u001b[0msentence1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msentence2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_match_potential_end_contexts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1430\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_break\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_match_potential_end_contexts\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1392\u001b[0m         \u001b[0mprevious_slice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1393\u001b[0m         \u001b[0mprevious_match\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1394\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1395\u001b[0m             \u001b[0;31m# Get the slice of the previous word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1396\u001b[0m             \u001b[0mbefore_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprevious_slice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object, got 'float'"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "df2 = df.copy()\n",
        "va = list(df2.columns)\n",
        "df_autres = df2[['Date de remplissage de la fiche','Autre raisons ,preciser',  'Si autres raison préciser']]\n",
        "#Extraction des tokens\n",
        "#Autres raisons\n",
        "df_autres['tokens indisponible']  =df_autres['Autre raisons ,preciser'].apply(nltk.word_tokenize)\n",
        "df_autres['tokens ineligible']  =df_autres['Si autres raison préciser'].apply(nltk.word_tokenize)\n",
        "print(\"Token List : \",df_autres.head())\n",
        "print(\"\\n Total Tokens : \",df_autres.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2019.columns"
      ],
      "metadata": {
        "id": "y0goa8itj_8T",
        "outputId": "eda312d9-031c-4925-ef02-981f2b7e4e1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Age', 'Date de remplissage de la fiche', 'Niveau d'etude', 'Genre',\n",
              "       'Taille', 'Poids', 'Situation Matrimoniale (SM)', 'Profession',\n",
              "       'Arrondissement de résidence', 'Quartier de Résidence', 'Nationalité',\n",
              "       'Religion', 'A-t-il (elle) déjà donné le sang',\n",
              "       'Si oui preciser la date du dernier don.', 'Taux d’hémoglobine',\n",
              "       'ÉLIGIBILITÉ AU DON.',\n",
              "       'Raison indisponibilité  [Est sous anti-biothérapie  ]',\n",
              "       'Raison indisponibilité  [Taux d’hémoglobine bas ]',\n",
              "       'Raison indisponibilité  [date de dernier Don < 3 mois ]',\n",
              "       'Raison indisponibilité  [IST récente (Exclu VIH, Hbs, Hcv)]',\n",
              "       'Date de dernières règles (DDR) ',\n",
              "       'Raison de l’indisponibilité de la femme [La DDR est mauvais si <14 jour avant le don]',\n",
              "       'Raison de l’indisponibilité de la femme [Allaitement ]',\n",
              "       'Raison de l’indisponibilité de la femme [A accoucher ces 6 derniers mois  ]',\n",
              "       'Raison de l’indisponibilité de la femme [Interruption de grossesse  ces 06 derniers mois]',\n",
              "       'Raison de l’indisponibilité de la femme [est enceinte ]',\n",
              "       'Autre raisons,  preciser', 'Sélectionner \"ok\" pour envoyer',\n",
              "       'Raison de non-eligibilité totale  [Antécédent de transfusion]',\n",
              "       'Raison de non-eligibilité totale  [Porteur(HIV,hbs,hcv)]',\n",
              "       'Raison de non-eligibilité totale  [Opéré]',\n",
              "       'Raison de non-eligibilité totale  [Drepanocytaire]',\n",
              "       'Raison de non-eligibilité totale  [Diabétique]',\n",
              "       'Raison de non-eligibilité totale  [Hypertendus]',\n",
              "       'Raison de non-eligibilité totale  [Asthmatiques]',\n",
              "       'Raison de non-eligibilité totale  [Cardiaque]',\n",
              "       'Raison de non-eligibilité totale  [Tatoué]',\n",
              "       'Raison de non-eligibilité totale  [Scarifié]',\n",
              "       'Si autres raison préciser'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\".isinstance('float')"
      ],
      "metadata": {
        "id": "uCZwgpprmldD",
        "outputId": "99322524-b55e-40cf-cc6e-06f3ff1585a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'isinstance'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-7b54ee240718>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'isinstance'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt_tab')  # Téléchargement du tokenizer de NLTK\n",
        "\n",
        "# Exemple de DataFrame\n",
        "df = pd.DataFrame({'text': [\"Bonjour, comment ça va ?\", \"Je fais du NLP avec Python !\"]})\n",
        "\n",
        "# Tokenisation\n",
        "df['tokens'] = df['text'].apply(word_tokenize)\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "id": "P3DO8oaBdcqD",
        "outputId": "04b34c80-d506-4cd8-f7b6-7b6dffe49349",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                           text                                tokens\n",
            "0      Bonjour, comment ça va ?      [Bonjour, ,, comment, ça, va, ?]\n",
            "1  Je fais du NLP avec Python !  [Je, fais, du, NLP, avec, Python, !]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "2tQWyIuld02M",
        "outputId": "5095f94d-7bf2-4651-8b4f-66bc8fad201c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClBZPD791oQI"
      },
      "source": [
        "## 03_02 Nettoyage du texte\n",
        "\n",
        "Nous verrons des exemples de suppression de ponctuation et de conversion en minuscules\n",
        "\n",
        "#### Supprimer la ponctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "QmzX4LGx1oQJ",
        "outputId": "aad45d54-4ca1-4f26-ec14-655296bc781b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'token_list' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-e824f47d2242>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Utiliser la bibliothèque Punkt pour extraire les jetons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtoken_list2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunkt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPunktToken\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_non_punct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Liste des tokens après suppression de la ponctuation : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtoken_list2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTotal des token après suppression de la ponctuation : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_list2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'token_list' is not defined"
          ]
        }
      ],
      "source": [
        "#Utiliser la bibliothèque Punkt pour extraire les jetons\n",
        "token_list2 = list(filter(lambda token: nltk.tokenize.punkt.PunktToken(token).is_non_punct, token_list))\n",
        "print(\"Liste des tokens après suppression de la ponctuation : \",token_list2[:20])\n",
        "print(\"\\nTotal des token après suppression de la ponctuation : \", len(token_list2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTfZPSbn1oQL"
      },
      "source": [
        "#### Convertir en minuscules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8zPUcMJ1oQM",
        "outputId": "3a5272a2-3cdc-41a1-8e38-b4cab09c84ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Liste des tokens après conversion en minuscules :  ['si', 'vous', 'vous', 'intéressez', 'au', 'big', 'data', 'vous', 'connaissez', 'certainement', 'apache', 'spark', 'savez-vous', 'pourquoi', 'spark', 'est', 'le', 'framework', 'de', 'prédilection']\n",
            "\n",
            "Total des tokens après conversion en minuscules :  87\n"
          ]
        }
      ],
      "source": [
        "token_list3=[word.lower() for word in token_list2 ]\n",
        "print(\"Liste des tokens après conversion en minuscules : \", token_list3[:20])\n",
        "print(\"\\nTotal des tokens après conversion en minuscules : \", len(token_list3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6rD3Cfh1oQO"
      },
      "source": [
        "## 03_03 Suppression des mots vides\n",
        "\n",
        "Suppression des mots vides à l'aide d'une liste de mots vides standard disponible dans NLTK pour l'anglais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hocIwm_y1oQP",
        "outputId": "f4ebdc40-2e14-4eee-fd43-f4feab569d13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Liste de tokens après suppression des mots vides :  ['si', 'intéressez', 'big', 'data', 'connaissez', 'certainement', 'apache', 'spark', 'savez-vous', 'pourquoi', 'spark', 'framework', 'prédilection', 'traitement', 'données', 'massives', 'pourquoi', 'est-il', 'autant', 'apprécié']\n",
            "\n",
            "Total de tokens après suppression des mots vides :  54\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#Download the standard stopword list\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#Remove stopwords\n",
        "token_list4 = list(filter(lambda token: token not in stopwords.words('french'), token_list3))\n",
        "print(\"Liste de tokens après suppression des mots vides : \", token_list4[:20])\n",
        "print(\"\\nTotal de tokens après suppression des mots vides : \", len(token_list4))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "CrR4WDo4eJ3i",
        "outputId": "97ad2a42-abff-4c5d-ab8d-8a46d8435c31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7vuOEIz1oQR"
      },
      "source": [
        "## 03_04 Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0skOV9G1oQS",
        "outputId": "e791faa7-2f6b-44d0-aa26-d98a5ff2a827"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Liste de tokens après le stemming :  ['si', 'intéress', 'big', 'dat', 'connaiss', 'certain', 'apach', 'spark', 'savez-vous', 'pourquoi', 'spark', 'framework', 'prédilect', 'trait', 'don', 'massiv', 'pourquoi', 'est-il', 'aut', 'appréci']\n",
            "\n",
            "Total de tokens après Stemming :  54\n"
          ]
        }
      ],
      "source": [
        "#Utilisez la bibliothèque SnowballStemmer pour la radicalisation.\n",
        "from nltk.stem import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"french\")\n",
        "\n",
        "#Stem data\n",
        "token_list5 = [stemmer.stem(word) for word in token_list4 ]\n",
        "print(\"Liste de tokens après le stemming : \", token_list5[:20])\n",
        "print(\"\\nTotal de tokens après Stemming : \", len(token_list5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE4TgYWz1oQU"
      },
      "source": [
        "## 03_05 Lemmatisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEPDLyk51oQV",
        "outputId": "801fd155-f3bf-4497-c030-241120930d57"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Liste de tokens après lemmatisation :  ['si', 'intéressez', 'big', 'data', 'connaissez', 'certainement', 'apache', 'spark', 'savez-vous', 'pourquoi', 'spark', 'framework', 'prédilection', 'traitement', 'données', 'massif', 'pourquoi', 'est-il', 'autant', 'apprécié']\n",
            "\n",
            "Total des tokens après lemmatisation :  54\n"
          ]
        }
      ],
      "source": [
        "#Utilisez la bibliothèque wordnet pour mapper les mots à leur forme lemmatisée\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "token_list6 = [lemmatizer.lemmatize(word) for word in token_list4 ]\n",
        "print(\"Liste de tokens après lemmatisation : \", token_list6[:20])\n",
        "print(\"\\nTotal des tokens après lemmatisation : \", len(token_list6))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skgZrdp_1oQW"
      },
      "source": [
        "#### Comparaison des tokens entre stemming et lemmatisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdyISdS71oQX",
        "outputId": "c86c7608-b95c-4a13-9f6c-fb050cf6cc7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raw :  notamment  , Stemmed :  not  , Lemmatized :  notamment\n"
          ]
        }
      ],
      "source": [
        "#Vérifier les technologies de jeton\n",
        "print( \"Raw : \", token_list4[20],\" , Stemmed : \", token_list5[20], \" , Lemmatized : \", token_list6[20])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUzt3Vxk1oQY"
      },
      "source": [
        "### Lemmatisation du text en français"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWmzkK6a1oQZ",
        "outputId": "21adc53f-1c09-4952-e855-d2449f881116"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "si si\n",
            "intéressez intéressez\n",
            "big big\n",
            "data data\n",
            "connaissez connaître\n",
            "certainement certainement\n",
            "apache apache\n",
            "spark spark\n",
            "savez savoir\n",
            "- -\n",
            "vous vous\n",
            "pourquoi pourquoi\n",
            "spark spark\n",
            "framework framework\n",
            "prédilection prédilection\n",
            "traitement traitement\n",
            "données donnée\n",
            "massives massif\n",
            "pourquoi pourquoi\n",
            "est-il est-il\n",
            "autant autant\n",
            "apprécié apprécier\n",
            "notamment notamment\n",
            "déployer déployer\n",
            "algorithmes algorithme\n",
            "machine machine\n",
            "learning learning\n",
            "découvrez découvrir\n",
            "cours cours\n",
            "apache apache\n",
            "pyspark pyspark\n",
            "répondre répondre\n",
            "toutes tout\n",
            "questions question\n",
            "travers travers\n",
            "multiples multiple\n",
            "exemples exemple\n",
            "mises mise\n",
            "pratique pratique\n",
            "professeur professeur\n",
            "associé associer\n",
            "technologies technologie\n",
            "l' le\n",
            "information information\n",
            "techniques technique\n",
            "d' de\n",
            "optimisation optimisation\n",
            "donne donne\n",
            "toutes tout\n",
            "clés clé\n",
            "analyser analyser\n",
            "efficacement efficacement\n",
            "données donner\n",
            "grande grand\n",
            "échelle échelle\n",
            "apache apache\n",
            "spark spark\n",
            "python python\n"
          ]
        }
      ],
      "source": [
        "# Plus d'information sur ce package : https://github.com/sammous/spacy-lefff\n",
        "\n",
        "\n",
        "# Pour installer spacy usiliser la commande \"pip install spacy\"\n",
        "# Il faut installer le dictionnaire français sur spacy \"python -m spacy download fr\"\n",
        "# Pour installer spacy lemmatiser utiliser la commande \"pip install spacy-lefff\"\n",
        "import spacy\n",
        "from spacy.language import Language\n",
        "\n",
        "\n",
        "nlp = spacy.load('fr_core_news_sm')\n",
        "# nlp.add_pipe('french_lemmatizer', name='lefff')\n",
        "doc = nlp(\" \".join(token_list4))\n",
        "for d in doc:\n",
        "    print(d.text, d.lemma_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LydWXttQ1oQa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}