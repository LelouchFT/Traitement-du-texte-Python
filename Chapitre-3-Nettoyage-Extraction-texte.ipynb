{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03_01 Tokenisation\n",
    "\n",
    "La tokenisation fait référence à la conversion d'une chaîne de texte en jetons individuels. Les jetons peuvent être des mots ou des ponctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token List :  ['Si', 'vous', 'vous', 'intéressez', 'au', 'big', 'data', ',', 'vous', 'connaissez', 'certainement', 'Apache', 'Spark', '.', 'Savez-vous', 'pourquoi', 'Spark', 'est', 'le', 'framework']\n",
      "\n",
      " Total Tokens :  95\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "\n",
    "\n",
    "#Lire le fichier de base dans une variable de texte brut\n",
    "base_file = open(os.getcwd()+ \"/Essentiel-Apache-Spark.txt\", 'rt')\n",
    "raw_text = base_file.read()\n",
    "base_file.close()\n",
    "\n",
    "#Extraire des tokens\n",
    "token_list = nltk.word_tokenize(raw_text)\n",
    "print(\"Token List : \",token_list[:20])\n",
    "print(\"\\n Total Tokens : \",len(token_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03_02 Nettoyage du texte\n",
    "\n",
    "Nous verrons des exemples de suppression de ponctuation et de conversion en minuscules\n",
    "\n",
    "#### Supprimer la ponctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste des tokens après suppression de la ponctuation :  ['Si', 'vous', 'vous', 'intéressez', 'au', 'big', 'data', 'vous', 'connaissez', 'certainement', 'Apache', 'Spark', 'Savez-vous', 'pourquoi', 'Spark', 'est', 'le', 'framework', 'de', 'prédilection']\n",
      "\n",
      "Total des token après suppression de la ponctuation :  87\n"
     ]
    }
   ],
   "source": [
    "#Utiliser la bibliothèque Punkt pour extraire les jetons\n",
    "token_list2 = list(filter(lambda token: nltk.tokenize.punkt.PunktToken(token).is_non_punct, token_list))\n",
    "print(\"Liste des tokens après suppression de la ponctuation : \",token_list2[:20])\n",
    "print(\"\\nTotal des token après suppression de la ponctuation : \", len(token_list2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convertir en minuscules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste des tokens après conversion en minuscules :  ['si', 'vous', 'vous', 'intéressez', 'au', 'big', 'data', 'vous', 'connaissez', 'certainement', 'apache', 'spark', 'savez-vous', 'pourquoi', 'spark', 'est', 'le', 'framework', 'de', 'prédilection']\n",
      "\n",
      "Total des tokens après conversion en minuscules :  87\n"
     ]
    }
   ],
   "source": [
    "token_list3=[word.lower() for word in token_list2 ]\n",
    "print(\"Liste des tokens après conversion en minuscules : \", token_list3[:20])\n",
    "print(\"\\nTotal des tokens après conversion en minuscules : \", len(token_list3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03_03 Suppression des mots vides\n",
    "\n",
    "Suppression des mots vides à l'aide d'une liste de mots vides standard disponible dans NLTK pour l'anglais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste de tokens après suppression des mots vides :  ['si', 'intéressez', 'big', 'data', 'connaissez', 'certainement', 'apache', 'spark', 'savez-vous', 'pourquoi', 'spark', 'framework', 'prédilection', 'traitement', 'données', 'massives', 'pourquoi', 'est-il', 'autant', 'apprécié']\n",
      "\n",
      "Total de tokens après suppression des mots vides :  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Download the standard stopword list\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Remove stopwords\n",
    "token_list4 = list(filter(lambda token: token not in stopwords.words('french'), token_list3))\n",
    "print(\"Liste de tokens après suppression des mots vides : \", token_list4[:20])\n",
    "print(\"\\nTotal de tokens après suppression des mots vides : \", len(token_list4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03_04 Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste de tokens après le stemming :  ['si', 'intéress', 'big', 'dat', 'connaiss', 'certain', 'apach', 'spark', 'savez-vous', 'pourquoi', 'spark', 'framework', 'prédilect', 'trait', 'don', 'massiv', 'pourquoi', 'est-il', 'aut', 'appréci']\n",
      "\n",
      "Total de tokens après Stemming :  54\n"
     ]
    }
   ],
   "source": [
    "#Utilisez la bibliothèque SnowballStemmer pour la radicalisation.\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"french\")\n",
    "\n",
    "#Stem data\n",
    "token_list5 = [stemmer.stem(word) for word in token_list4 ]\n",
    "print(\"Liste de tokens après le stemming : \", token_list5[:20])\n",
    "print(\"\\nTotal de tokens après Stemming : \", len(token_list5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03_05 Lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste de tokens après lemmatisation :  ['si', 'intéressez', 'big', 'data', 'connaissez', 'certainement', 'apache', 'spark', 'savez-vous', 'pourquoi', 'spark', 'framework', 'prédilection', 'traitement', 'données', 'massif', 'pourquoi', 'est-il', 'autant', 'apprécié']\n",
      "\n",
      "Total des tokens après lemmatisation :  54\n"
     ]
    }
   ],
   "source": [
    "#Utilisez la bibliothèque wordnet pour mapper les mots à leur forme lemmatisée\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "token_list6 = [lemmatizer.lemmatize(word) for word in token_list4 ]\n",
    "print(\"Liste de tokens après lemmatisation : \", token_list6[:20])\n",
    "print(\"\\nTotal des tokens après lemmatisation : \", len(token_list6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparaison des tokens entre stemming et lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw :  notamment  , Stemmed :  not  , Lemmatized :  notamment\n"
     ]
    }
   ],
   "source": [
    "#Vérifier les technologies de jeton\n",
    "print( \"Raw : \", token_list4[20],\" , Stemmed : \", token_list5[20], \" , Lemmatized : \", token_list6[20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatisation du text en français"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "si si\n",
      "intéressez intéressez\n",
      "big big\n",
      "data data\n",
      "connaissez connaître\n",
      "certainement certainement\n",
      "apache apache\n",
      "spark spark\n",
      "savez savoir\n",
      "- -\n",
      "vous vous\n",
      "pourquoi pourquoi\n",
      "spark spark\n",
      "framework framework\n",
      "prédilection prédilection\n",
      "traitement traitement\n",
      "données donnée\n",
      "massives massif\n",
      "pourquoi pourquoi\n",
      "est-il est-il\n",
      "autant autant\n",
      "apprécié apprécier\n",
      "notamment notamment\n",
      "déployer déployer\n",
      "algorithmes algorithme\n",
      "machine machine\n",
      "learning learning\n",
      "découvrez découvrir\n",
      "cours cours\n",
      "apache apache\n",
      "pyspark pyspark\n",
      "répondre répondre\n",
      "toutes tout\n",
      "questions question\n",
      "travers travers\n",
      "multiples multiple\n",
      "exemples exemple\n",
      "mises mise\n",
      "pratique pratique\n",
      "professeur professeur\n",
      "associé associer\n",
      "technologies technologie\n",
      "l' le\n",
      "information information\n",
      "techniques technique\n",
      "d' de\n",
      "optimisation optimisation\n",
      "donne donne\n",
      "toutes tout\n",
      "clés clé\n",
      "analyser analyser\n",
      "efficacement efficacement\n",
      "données donner\n",
      "grande grand\n",
      "échelle échelle\n",
      "apache apache\n",
      "spark spark\n",
      "python python\n"
     ]
    }
   ],
   "source": [
    "# Plus d'information sur ce package : https://github.com/sammous/spacy-lefff\n",
    "\n",
    "\n",
    "# Pour installer spacy usiliser la commande \"pip install spacy\"\n",
    "# Il faut installer le dictionnaire français sur spacy \"python -m spacy download fr\"\n",
    "# Pour installer spacy lemmatiser utiliser la commande \"pip install spacy-lefff\"\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "# nlp.add_pipe('french_lemmatizer', name='lefff')\n",
    "doc = nlp(\" \".join(token_list4))\n",
    "for d in doc:\n",
    "    print(d.text, d.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
