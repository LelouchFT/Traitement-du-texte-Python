{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-bcHjIs1oP1"
      },
      "source": [
        "## 03_01 Tokenisation\n",
        "\n",
        "La tokenisation fait référence à la conversion d'une chaîne de texte en jetons individuels. Les jetons peuvent être des mots ou des ponctuations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "c0TZBmvZ1oP_",
        "outputId": "22268a75-cfc9-4767-9f0c-2c49e2454ed8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'file_path' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-30f9bcfe63ff>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#Lire le fichier de base dans une variable de texte brut\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheet_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdf2019\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'2019'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'file_path' is not defined"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#Lire le fichier de base dans une variable de texte brut\n",
        "dfs = pd.read_excel(file_path, sheet_name=None)\n",
        "df2019 = dfs['2019']\n",
        "\n",
        "# Convertir les dates en format datetime et normaliser\n",
        "df2019['Date de remplissage de la fiche'] = pd.to_datetime(df2019['Date de remplissage de la fiche'], errors='coerce').dt.normalize()\n",
        "\n",
        "df2019['Date de remplissage de la fiche'] = df2019['Date de remplissage de la fiche'].fillna(df2019['Date de remplissage de la fiche'].mode()[0])\n",
        "df2019['Date de naissance'] = pd.to_datetime(df2019['Date de naissance'], errors='coerce').dt.normalize()\n",
        "df2019['Date de naissance'] = df2019['Date de naissance'].fillna(df2019['Date de naissance'].mode()[0])\n",
        "df2019['Si oui preciser la date du dernier don.'] = pd.to_datetime(df2019['Si oui preciser la date du dernier don.'], errors='coerce').dt.normalize()\n",
        "# Calculer l'âge s\n",
        "\n",
        "df2019['Age'] = df2019.apply( lambda x: x['Date de remplissage de la fiche'].year - x['Date de naissance'].year ,\n",
        "    axis=1\n",
        ").astype('Int64')\n",
        "\n",
        "# Afficher les 5 premières valeurs de la colonne Age\n",
        "ages = df2019.pop('Age')\n",
        "df2019.insert(0,'Age',ages)\n",
        "df2 = df2019.copy()\n",
        "#print(df2019[['Age','Date de remplissage de la fiche','Date de naissance']].head(15))\n",
        "df2019 = df2019.drop('Date de naissance', axis =1)\n",
        "df_vol = dfs['Volontaire']\n",
        "df_vol = df_vol.drop(['ID','Horodateur'],axis = 1)\n",
        "df_vol['Date de remplissage de la fiche'] = np.nan\n",
        "df_vol.columns = df2019.columns\n",
        "\n",
        "\n",
        "df = pd.concat([df_vol,df2019],ignore_index = True)\n",
        "imputer = SimpleImputer(strategy = 'constant',fill_value = 0)\n",
        "df.columns = ['Age',\"Niveau d'etude\", 'Genre', 'Taille', 'Poids',\n",
        "       'Situation Matrimoniale (SM)', 'Profession',\n",
        "       'Arrondissement de résidence', 'Quartier de Résidence', 'Nationalité',\n",
        "       'Religion', 'A-t-il (elle) déjà donné le sang',\n",
        "       'Si oui preciser la date du dernier don.', 'Taux d’hémoglobine',\n",
        "       'ÉLIGIBILITÉ AU DON.',\n",
        "       'Est sous anti-biothérapie',\n",
        "       'Taux d’hémoglobine bas',\n",
        "       'date de dernier Don < 3 mois',\n",
        "       'IST récente (Exclu VIH, Hbs, Hcv)',\n",
        "       'DDR',\n",
        "       'DDR<14',\n",
        "       'Allaitement',\n",
        "       'A accoucher ces 6 derniers mois',\n",
        "       'Interruption de grossesse  ces 06 derniers mois',\n",
        "       'est enceinte',\n",
        "       'Autre raisons ,preciser', 'Sélectionner \"ok\" pour envoyer',\n",
        "       'Antécédent de transfusion',\n",
        "       'Porteur(HIV,hbs,hcv)',\n",
        "       'Opéré',\n",
        "       'Drepanocytaire',       'Diabétique',\n",
        "       'Hypertendus',\n",
        "       'Asthmatiques',\n",
        "       'Cardiaque',\n",
        "       'Tatoué',\n",
        "       'Scarifié',\n",
        "       'Si autres raison préciser']\n",
        "\n",
        "\n",
        "col = ['ÉLIGIBILITÉ AU DON.',\n",
        "       'Est sous anti-biothérapie',\n",
        "       'Taux d’hémoglobine bas',\n",
        "       'date de dernier Don < 3 mois',\n",
        "       'IST récente (Exclu VIH, Hbs, Hcv)',\n",
        "       'DDR<14',\n",
        "       'Allaitement',\n",
        "       'A accoucher ces 6 derniers mois',\n",
        "       'Interruption de grossesse  ces 06 derniers mois',\n",
        "       'est enceinte',\n",
        "       'Antécédent de transfusion',\n",
        "       'Porteur(HIV,hbs,hcv)',\n",
        "       'Opéré',\n",
        "       'Drepanocytaire',\n",
        "       'Diabétique',\n",
        "       'Hypertendus',\n",
        "       'Asthmatiques',\n",
        "       'Cardiaque',\n",
        "       'Tatoué',\n",
        "       'Scarifié'\n",
        "       ]\n",
        "for c in col[1:] :\n",
        "    df[c] = df[c].apply(lambda x : 1 if x == 'Oui' else 0 )\n",
        "df['ÉLIGIBILITÉ AU DON.'] = df['ÉLIGIBILITÉ AU DON.'].apply(lambda x : 1 if x == 'Eligible' else 2 if x == 'Temporairement Non-eligible' else 0 )\n",
        "df_cat = df[col]\n",
        "col = df_cat.columns\n",
        "df_cat = imputer.fit_transform(df_cat)\n",
        "df_cat = pd.DataFrame(df_cat, columns = col)\n",
        "\n",
        "\n",
        "df_autres = df['Autre raisons ,preciser','Si autres raison préciser'].copy().dropna()\n",
        "#Extraction des tokens\n",
        "df_autres['tokens indisponible']  =df_autres['Autre raisons ,preciser'].apply(lambda x : nltk.word_tokenize(x))\n",
        "print(\"Token List : \",token_list[:20])\n",
        "print(\"\\n Total Tokens : \",len(token_list))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt_tab')  # Téléchargement du tokenizer de NLTK\n",
        "\n",
        "# Exemple de DataFrame\n",
        "df = pd.DataFrame({'text': [\"Bonjour, comment ça va ?\", \"Je fais du NLP avec Python !\"]})\n",
        "\n",
        "# Tokenisation\n",
        "df['tokens'] = df['text'].apply(word_tokenize)\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "id": "P3DO8oaBdcqD",
        "outputId": "04b34c80-d506-4cd8-f7b6-7b6dffe49349",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                           text                                tokens\n",
            "0      Bonjour, comment ça va ?      [Bonjour, ,, comment, ça, va, ?]\n",
            "1  Je fais du NLP avec Python !  [Je, fais, du, NLP, avec, Python, !]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "2tQWyIuld02M",
        "outputId": "5095f94d-7bf2-4651-8b4f-66bc8fad201c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClBZPD791oQI"
      },
      "source": [
        "## 03_02 Nettoyage du texte\n",
        "\n",
        "Nous verrons des exemples de suppression de ponctuation et de conversion en minuscules\n",
        "\n",
        "#### Supprimer la ponctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "scrolled": true,
        "id": "QmzX4LGx1oQJ",
        "outputId": "aad45d54-4ca1-4f26-ec14-655296bc781b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'token_list' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-e824f47d2242>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Utiliser la bibliothèque Punkt pour extraire les jetons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtoken_list2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunkt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPunktToken\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_non_punct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Liste des tokens après suppression de la ponctuation : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtoken_list2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTotal des token après suppression de la ponctuation : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_list2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'token_list' is not defined"
          ]
        }
      ],
      "source": [
        "#Utiliser la bibliothèque Punkt pour extraire les jetons\n",
        "token_list2 = list(filter(lambda token: nltk.tokenize.punkt.PunktToken(token).is_non_punct, token_list))\n",
        "print(\"Liste des tokens après suppression de la ponctuation : \",token_list2[:20])\n",
        "print(\"\\nTotal des token après suppression de la ponctuation : \", len(token_list2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTfZPSbn1oQL"
      },
      "source": [
        "#### Convertir en minuscules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8zPUcMJ1oQM",
        "outputId": "3a5272a2-3cdc-41a1-8e38-b4cab09c84ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Liste des tokens après conversion en minuscules :  ['si', 'vous', 'vous', 'intéressez', 'au', 'big', 'data', 'vous', 'connaissez', 'certainement', 'apache', 'spark', 'savez-vous', 'pourquoi', 'spark', 'est', 'le', 'framework', 'de', 'prédilection']\n",
            "\n",
            "Total des tokens après conversion en minuscules :  87\n"
          ]
        }
      ],
      "source": [
        "token_list3=[word.lower() for word in token_list2 ]\n",
        "print(\"Liste des tokens après conversion en minuscules : \", token_list3[:20])\n",
        "print(\"\\nTotal des tokens après conversion en minuscules : \", len(token_list3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6rD3Cfh1oQO"
      },
      "source": [
        "## 03_03 Suppression des mots vides\n",
        "\n",
        "Suppression des mots vides à l'aide d'une liste de mots vides standard disponible dans NLTK pour l'anglais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hocIwm_y1oQP",
        "outputId": "f4ebdc40-2e14-4eee-fd43-f4feab569d13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Liste de tokens après suppression des mots vides :  ['si', 'intéressez', 'big', 'data', 'connaissez', 'certainement', 'apache', 'spark', 'savez-vous', 'pourquoi', 'spark', 'framework', 'prédilection', 'traitement', 'données', 'massives', 'pourquoi', 'est-il', 'autant', 'apprécié']\n",
            "\n",
            "Total de tokens après suppression des mots vides :  54\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#Download the standard stopword list\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#Remove stopwords\n",
        "token_list4 = list(filter(lambda token: token not in stopwords.words('french'), token_list3))\n",
        "print(\"Liste de tokens après suppression des mots vides : \", token_list4[:20])\n",
        "print(\"\\nTotal de tokens après suppression des mots vides : \", len(token_list4))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "CrR4WDo4eJ3i",
        "outputId": "97ad2a42-abff-4c5d-ab8d-8a46d8435c31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7vuOEIz1oQR"
      },
      "source": [
        "## 03_04 Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0skOV9G1oQS",
        "outputId": "e791faa7-2f6b-44d0-aa26-d98a5ff2a827"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Liste de tokens après le stemming :  ['si', 'intéress', 'big', 'dat', 'connaiss', 'certain', 'apach', 'spark', 'savez-vous', 'pourquoi', 'spark', 'framework', 'prédilect', 'trait', 'don', 'massiv', 'pourquoi', 'est-il', 'aut', 'appréci']\n",
            "\n",
            "Total de tokens après Stemming :  54\n"
          ]
        }
      ],
      "source": [
        "#Utilisez la bibliothèque SnowballStemmer pour la radicalisation.\n",
        "from nltk.stem import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"french\")\n",
        "\n",
        "#Stem data\n",
        "token_list5 = [stemmer.stem(word) for word in token_list4 ]\n",
        "print(\"Liste de tokens après le stemming : \", token_list5[:20])\n",
        "print(\"\\nTotal de tokens après Stemming : \", len(token_list5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE4TgYWz1oQU"
      },
      "source": [
        "## 03_05 Lemmatisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEPDLyk51oQV",
        "outputId": "801fd155-f3bf-4497-c030-241120930d57"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Liste de tokens après lemmatisation :  ['si', 'intéressez', 'big', 'data', 'connaissez', 'certainement', 'apache', 'spark', 'savez-vous', 'pourquoi', 'spark', 'framework', 'prédilection', 'traitement', 'données', 'massif', 'pourquoi', 'est-il', 'autant', 'apprécié']\n",
            "\n",
            "Total des tokens après lemmatisation :  54\n"
          ]
        }
      ],
      "source": [
        "#Utilisez la bibliothèque wordnet pour mapper les mots à leur forme lemmatisée\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "token_list6 = [lemmatizer.lemmatize(word) for word in token_list4 ]\n",
        "print(\"Liste de tokens après lemmatisation : \", token_list6[:20])\n",
        "print(\"\\nTotal des tokens après lemmatisation : \", len(token_list6))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skgZrdp_1oQW"
      },
      "source": [
        "#### Comparaison des tokens entre stemming et lemmatisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdyISdS71oQX",
        "outputId": "c86c7608-b95c-4a13-9f6c-fb050cf6cc7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raw :  notamment  , Stemmed :  not  , Lemmatized :  notamment\n"
          ]
        }
      ],
      "source": [
        "#Vérifier les technologies de jeton\n",
        "print( \"Raw : \", token_list4[20],\" , Stemmed : \", token_list5[20], \" , Lemmatized : \", token_list6[20])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUzt3Vxk1oQY"
      },
      "source": [
        "### Lemmatisation du text en français"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWmzkK6a1oQZ",
        "outputId": "21adc53f-1c09-4952-e855-d2449f881116"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "si si\n",
            "intéressez intéressez\n",
            "big big\n",
            "data data\n",
            "connaissez connaître\n",
            "certainement certainement\n",
            "apache apache\n",
            "spark spark\n",
            "savez savoir\n",
            "- -\n",
            "vous vous\n",
            "pourquoi pourquoi\n",
            "spark spark\n",
            "framework framework\n",
            "prédilection prédilection\n",
            "traitement traitement\n",
            "données donnée\n",
            "massives massif\n",
            "pourquoi pourquoi\n",
            "est-il est-il\n",
            "autant autant\n",
            "apprécié apprécier\n",
            "notamment notamment\n",
            "déployer déployer\n",
            "algorithmes algorithme\n",
            "machine machine\n",
            "learning learning\n",
            "découvrez découvrir\n",
            "cours cours\n",
            "apache apache\n",
            "pyspark pyspark\n",
            "répondre répondre\n",
            "toutes tout\n",
            "questions question\n",
            "travers travers\n",
            "multiples multiple\n",
            "exemples exemple\n",
            "mises mise\n",
            "pratique pratique\n",
            "professeur professeur\n",
            "associé associer\n",
            "technologies technologie\n",
            "l' le\n",
            "information information\n",
            "techniques technique\n",
            "d' de\n",
            "optimisation optimisation\n",
            "donne donne\n",
            "toutes tout\n",
            "clés clé\n",
            "analyser analyser\n",
            "efficacement efficacement\n",
            "données donner\n",
            "grande grand\n",
            "échelle échelle\n",
            "apache apache\n",
            "spark spark\n",
            "python python\n"
          ]
        }
      ],
      "source": [
        "# Plus d'information sur ce package : https://github.com/sammous/spacy-lefff\n",
        "\n",
        "\n",
        "# Pour installer spacy usiliser la commande \"pip install spacy\"\n",
        "# Il faut installer le dictionnaire français sur spacy \"python -m spacy download fr\"\n",
        "# Pour installer spacy lemmatiser utiliser la commande \"pip install spacy-lefff\"\n",
        "import spacy\n",
        "from spacy.language import Language\n",
        "\n",
        "\n",
        "nlp = spacy.load('fr_core_news_sm')\n",
        "# nlp.add_pipe('french_lemmatizer', name='lefff')\n",
        "doc = nlp(\" \".join(token_list4))\n",
        "for d in doc:\n",
        "    print(d.text, d.lemma_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LydWXttQ1oQa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}